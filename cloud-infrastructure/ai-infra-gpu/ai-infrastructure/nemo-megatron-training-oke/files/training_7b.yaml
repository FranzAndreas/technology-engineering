apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  annotations:
  name: nemo-megatron-training
spec:
  minAvailable: 0
  plugins:
    ssh: []
    svc: []
  queue: default
  schedulerName: volcano
  tasks:
  - name: mpimaster
    policies:
    - action: CompleteJob
      event: TaskCompleted
    replicas: 1
    template:
      metadata:
      spec:
        containers:
        - command:
          - /bin/bash
          - -c
          - |
            set -e -o pipefail; trap 'exit=1' SIGINT
            NUM_GPUS=8
            NUM_HOSTS=$(sed -n '$=' /etc/volcano/mpiworker.host)
            NP=$(($NUM_HOSTS*$NUM_GPUS))

            HOSTFILE=$PWD/myhosts.sorted
            bash /mnt/data/utils/sort_hosts.sh $HOSTFILE

            mpirun --allow-run-as-root \
              -mca plm_rsh_args "-p 2222" \
              -np $NP -npernode $NUM_GPUS --bind-to numa \
              -hostfile $HOSTFILE \
              -x NVIDIA_PYTORCH_VERSION \
              -x PYTHONPATH \
              -x HOST_LIST \
              $MPI_ARGS \
              bash -c "
                # c10d will publish a host address based on the OCI hostname, not
                # the labelling of OKE. We don't seem to have DNS mapping, manually
                # do that for now.
                [[ \${HOST_LIST:+yes} == yes ]] && echo \"\${HOST_LIST}\" >> /etc/hosts

                export HYDRA_FULL_ERROR=1
                export CUDA_DEVICE_MAX_CONNECTIONS=1
                export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python

                # From NVIDIA SLURM reference setup;  These lead to performance
                # regression.
                # export TORCH_NCCL_AVOID_RECORD_STREAMS=1
                # export NCCL_NVLS_ENABLE=0
                # export NVTE_DP_AMAX_REDUCE_INTERVAL=0
                # export NVTE_ASYNC_AMAX_REDUCTION=1
                # export NVTE_FUSED_ATTN=0

                # Gloo connectFullMesh failed
                export GLOO_SOCKET_IFNAME=eth0
                export TP_SOCKET_IFNAME=eth0

                python -u /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \
                    --config-path=/mnt/data/config \
                    --config-name=config_7b.yaml"
            while :; do { [[ $exit ]] && break; }; sleep 1; done
          ports:
          - { name: mpijob-port, containerPort: 2222, protocol: TCP }
          image: nvcr.io/nvidia/nemo:24.09
          name: mpimaster
          env:
          - name: OMP_NUM_THREADS
            value: "14"
          envFrom:
          - configMapRef:
              name: mpi-setup
          resources:
            limits:
              ephemeral-storage: 16Gi
            requests:
              cpu: 4
              ephemeral-storage: 16Gi
              memory: 1Gi
          securityContext:
            privileged: true
            capabilities:
              add:
              - IPC_LOCK
          volumeMounts:
          - { mountPath: /dev/infiniband, name: devinf }
          - { mountPath: /dev/shm, name: shm }
          - { mountPath: /mnt/data, name: workspace, readOnly: false }
          workingDir: /workspace
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        restartPolicy: OnFailure
        terminationGracePeriodSeconds: 2
        volumes:
        - { name: devinf, hostPath: { path: /dev/infiniband }}
        - { name: shm, emptyDir: { medium: Memory, sizeLimit: 32Gi }}
        - { name: workspace, persistentVolumeClaim: { claimName: fss-pv }}
  - name: mpiworker
    minAvailable: 0
    replicas: 2
    template:
      metadata:
      spec:
        containers:
        - command:
          - /bin/bash
          - -c
          - mkdir -p /var/run/sshd; /usr/sbin/sshd -D -p 2222 || sleep 999999999;
          image: nvcr.io/nvidia/nemo:24.09
          name: mpiworker
          ports:
          - { name: mpijob-port, containerPort: 2222, protocol: TCP }
          - { name: c10d, containerPort: 29500, protocol: TCP }
          resources:
            limits:
              ephemeral-storage: 32Gi
              nvidia.com/gpu: 8
            requests:
              cpu: 112
              ephemeral-storage: 32Gi
              memory: 768Gi
              nvidia.com/gpu: 8
          securityContext:
            privileged: true
            capabilities:
              add:
              - IPC_LOCK
              - CAP_SYS_ADMIN
          volumeMounts:
          - { mountPath: /dev/infiniband, name: devinf }
          - { mountPath: /dev/shm, name: shm }
          - { mountPath: /mnt/data, name: workspace, readOnly: false }
          workingDir: /workspace
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        restartPolicy: OnFailure
        terminationGracePeriodSeconds: 15
        tolerations:
        - { key: nvidia.com/gpu, operator: Exists }
        volumes:
        - { name: devinf, hostPath: { path: /dev/infiniband }}
        - { name: shm, emptyDir: { medium: Memory, sizeLimit: 32Gi }}
        - { name: workspace, persistentVolumeClaim: { claimName: fss-pv }}
